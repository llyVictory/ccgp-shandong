import requests
import json
import base64
import time
from bs4 import BeautifulSoup
import pandas as pd
import threading
from concurrent.futures import ThreadPoolExecutor

import random

class Shandong(object):
    def __init__(self, use_proxy=False):
        self.list_url = "http://www.ccgp-shandong.gov.cn:8087/api/website/site/getListByCode"
        self.detail_url = "http://www.ccgp-shandong.gov.cn:8087/api/website/site/getDetail"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        ]
        self.colCode = "2500" # æ”¿é‡‡æ„å‘
        
        self.use_proxy = use_proxy
        self.proxies = None
        if self.use_proxy:
            self.proxies = {
                "http": "http://127.0.0.1:7897",
                "https": "http://127.0.0.1:7897",
            }
        
        self.log_func = None
        
        # ä»…åœ¨å¯ç”¨ä»£ç†æ—¶æ£€æŸ¥çŠ¶æ€
        if self.use_proxy:
            self.check_proxy()
        else:
            self._log("="*50)
            self._log("âš ï¸ ä»£ç†å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°ç›´æ¥è¿æ¥ã€‚")
            self._log("="*50)

    def check_proxy(self):
        """æ£€æŸ¥ä»£ç†æ˜¯å¦ç”Ÿæ•ˆå¹¶è·å–å‡ºå£IPä½ç½®"""
        self._log("="*50)
        self._log("æ­£åœ¨æ£€æŸ¥ç½‘ç»œå‡ºå£ç¯å¢ƒ...")
        test_url = "http://ip-api.com/json?lang=zh-CN"
        proxies = self.proxies
        
        try:
            # 1. è·å–ä»£ç†å‡ºå£ä¿¡æ¯
            resp = requests.get(test_url, proxies=proxies, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                ip = data.get("query")
                country = data.get("country", "")
                region = data.get("regionName", "")
                city = data.get("city", "")
                isp = data.get("isp", "")
                
                self._log(f"âœ… ä»£ç†å·²ç”Ÿæ•ˆï¼")
                self._log(f"   å½“å‰æ¢æµ‹å‡ºå£ IP: {ip}")
                self._log(f"   ç‰©ç†åœ°ç†ä½ç½®: {country} - {region} - {city}")
                self._log(f"   è¿è¥å•†ä¿¡æ¯: {isp}")
            else:
                self._log(f"âš ï¸ ä»£ç†è¿æ¥æµ‹è¯•è¿”å›çŠ¶æ€ç : {resp.status_code}")
        except Exception as e:
            self._log(f"âŒ ä»£ç†è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ Clash (127.0.0.1:7897) æ˜¯å¦å¼€å¯ã€‚")
            self._log(f"   é”™è¯¯è¯¦æƒ…: {e}")
        
        self._log("="*50)

    def _log(self, msg):
        if self.log_func:
            self.log_func(msg)
        else:
            print(msg)

    def get_headers(self):
        return {
            "accept": "application/json, text/plain, */*",
            # "accept-encoding": "gzip, deflate", # requests usually handles this
            "accept-language": "zh-CN,zh;q=0.9",
            "connection": "keep-alive",
            "content-type": "application/json;charset=UTF-8",
            "host": "www.ccgp-shandong.gov.cn:8087",
            "origin": "http://www.ccgp-shandong.gov.cn",
            "referer": "http://www.ccgp-shandong.gov.cn/",
            "user-agent": random.choice(self.user_agents)
        }
    def get_list(self, page, title="", start_time="", end_time="", area="370000"):
        # Date format must be YYYY-MM-DD HH:mm:ss
        if start_time and len(start_time) == 10:
            start_time += " 00:00:00"
        if end_time and len(end_time) == 10:
            end_time += " 23:59:59"
            
        data = {
            "colCode": self.colCode,
            "area": area,
            "currentPage": page,
            "pageSize": 10,
            "title": title,
            "projectCode": "",
            "buyKind": "",
            "buyType": "",
            "startTime": start_time if start_time else "",
            "oldData": 0,
            "endTime": end_time if end_time else "",
            "homePage": 0,
            "mergeType": 0
        }
        try:
            self._log(f"æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ (åœ°åŒº: {area}, æœç´¢è¯: {title})")
            # ä¸¥æ ¼åçˆ¬ï¼šåˆ—è¡¨é¡µè¯·æ±‚å‰éšæœºä¼‘çœ  2-5 ç§’
            time.sleep(random.uniform(2.0, 5.0))
            
            resp = requests.post(self.list_url, json=data, headers=self.get_headers(), timeout=20, proxies=self.proxies)
            
            # çŠ¶æ€ç ç›‘æ§
            if resp.status_code in [403, 429]:
                self._log("ğŸ”¥ è­¦å‘Š: è§¦å‘æœåŠ¡å™¨æ‹¦æˆª (403/429)ï¼Œç«‹å³åœæ­¢çˆ¬å–ä»¥ä¿æŠ¤ IPï¼")
                return [], -1
            elif resp.status_code >= 500:
                self._log(f"ğŸ”¥ è­¦å‘Š: ç›®æ ‡æœåŠ¡å™¨è¿‡è½½æˆ–å‡ºé”™ (é”™è¯¯ç : {resp.status_code})ï¼Œåœæ­¢çˆ¬å–ï¼Œé¿å…åŠ é‡è´Ÿæ‹…ï¼")
                return [], -1
                
            if resp.status_code == 200:
                j = resp.json()
                # Assuming structure: j['data']['data']['records'] based on investigation
                # But test_api.py output showed j['data']['data'] has 'records'
                # Let's handle both just in case or stick to what we saw.
                if j.get("data") and j["data"].get("data") and j["data"]["data"].get("records"):
                    return j["data"]["data"]["records"], j["data"]["data"].get("pages", 0)
                else:
                    self._log("Debug - API JSON structure: " + json.dumps(j, indent=2, ensure_ascii=False))
            else:
                self._log(f"List error page {page}, status {resp.status_code}: {resp.text}")
        except Exception as e:
            self._log(f"List exception page {page}: {e}")
        return [], 0

    def get_detail_html(self, id_val, colCode):
        params = {
            "id": id_val,
            "colCode": colCode,
            "oldData": 0
        }
        try:
            # ä¸¥æ ¼åçˆ¬ï¼šè¯¦æƒ…é¡µè¯·æ±‚å‰éšæœºä¼‘çœ  2-5 ç§’
            time.sleep(random.uniform(2.0, 5.0))
            resp = requests.get(self.detail_url, params=params, headers=self.get_headers(), timeout=20, proxies=self.proxies)
            
            if resp.status_code in [403, 429]:
                self._log(f"ğŸ”¥ è¯¦æƒ…é¡µ {id_val} è§¦å‘æ‹¦æˆªï¼Œè·³è¿‡...")
                return None
                
            if resp.status_code == 200:
                j = resp.json()
                if j.get("data") and j["data"].get("data") and j["data"]["data"].get("body"):
                    body = j["data"]["data"]["body"]
                    try:
                        return base64.b64decode(body).decode('utf-8')
                    except:
                        try:
                            return base64.b64decode(body).decode('gb18030')
                        except:
                            return None
        except Exception as e:
            self._log(f"Detail exception {id_val}: {e}")
        return None

    def parse_html_table(self, html):
        """
        [V3.1] ç»ˆæè§£ææ–¹æ¡ˆï¼šè‡ªåŠ¨çº é”™ä¸å»é‡ (Fixed)
        1. å¿…é¡»åŒ…å« 'åºå·' åˆ—æ‰è§†ä¸ºæœ‰æ•ˆæ¸…å•è¡¨ã€‚
        2. ä½¿ç”¨ recursive=False å¹¶æ”¯æŒ tbody æŸ¥æ‰¾ã€‚
        3. æ™ºèƒ½åˆ—åç§»æ ¡æ­£ï¼šæ£€æµ‹åˆ°â€œåºå·â€åˆ—ç”±é•¿æ–‡æœ¬å æ®æ—¶ï¼Œè‡ªåŠ¨è§¦å‘ Left-Shift ä¿®æ­£ã€‚
        4. å…¨å±€å»é‡ï¼šé˜²æ­¢åŒä¸€é¡¹ç›®è¢«å¤šæ¬¡æå–ã€‚
        """
        if not html:
            return []
        soup = BeautifulSoup(html, 'lxml')
        tables = soup.find_all('table')
        results = []
        seen_titles = set()
        
        self._log(f"Debug: Found {len(tables)} tables")
        
        for table_idx, table in enumerate(tables):
            # ä¼˜å…ˆæŸ¥æ‰¾ç›´æ¥å­èŠ‚ç‚¹ trï¼Œè‹¥æ— åˆ™æŸ¥æ‰¾ tbody ä¸‹çš„ tr
            rows = table.find_all('tr', recursive=False)
            if len(rows) < 2: 
                tbody = table.find('tbody', recursive=False)
                if tbody:
                    rows = tbody.find_all('tr', recursive=False)
            
            if len(rows) < 2: 
                continue
            
            # 1. ç²¾ç¡®å¯»æ‰¾è¡¨å¤´è¡Œ
            header_row_idx = -1
            col_map = {
                "sub_index": -1, "project_name": -1, "desc": -1, 
                "amount": -1, "sme_reserve": -1, "est_time": -1, "remark": -1
            }
            
            # æœç´¢å‰ 6 è¡Œå¯»æ‰¾è¡¨å¤´
            for idx, tr in enumerate(rows[:6]):
                cells = tr.find_all(['td', 'th'], recursive=False)
                headers = [c.get_text(strip=True) for c in cells]
                
                temp_map = {k: -1 for k in col_map}
                for i, h in enumerate(headers):
                    if "åºå·" in h: temp_map["sub_index"] = i
                    elif "åç§°" in h: temp_map["project_name"] = i
                    elif "æ¦‚å†µ" in h or "éœ€æ±‚" in h: temp_map["desc"] = i
                    elif "é‡‘é¢" in h: temp_map["amount"] = i
                    elif "ä¸­å°ä¼ä¸š" in h: temp_map["sme_reserve"] = i
                    elif "æ—¶é—´" in h: temp_map["est_time"] = i
                    elif "å¤‡æ³¨" in h: temp_map["remark"] = i
                
                # ä¸¥æ ¼æ ‡å‡†ï¼šå¿…é¡»æ‰¾åˆ°â€œåºå·â€å’Œâ€œé¡¹ç›®åç§°â€æ‰è§†ä¸ºæœ‰æ•ˆè¡¨å¤´
                if temp_map["sub_index"] != -1 and temp_map["project_name"] != -1:
                    header_row_idx = idx
                    col_map = temp_map
                    break
            
            if header_row_idx == -1:
                continue

            # 2. ä»è¡¨å¤´ä¸‹ä¸€è¡Œå¼€å§‹éå†æ•°æ®
            for row in rows[header_row_idx+1:]:
                cols = row.find_all(['td', 'th'], recursive=False)
                if len(cols) < 2: continue
                
                def get_clean_text(idx):
                    if idx != -1 and idx < len(cols):
                        txt = cols[idx].get_text(" ", strip=True) # ä½¿ç”¨ç©ºæ ¼è¿æ¥æ ‡ç­¾å†…å®¹
                        txt = txt.replace("\n", " ").replace("\r", " ").replace("\t", " ")
                        while "  " in txt: 
                            txt = txt.replace("  ", " ")
                        return txt.strip()
                    return ""

                # æå–åŸå§‹æ•°æ®
                raw_idx_val = get_clean_text(col_map["sub_index"])
                raw_name_val = get_clean_text(col_map["project_name"])
                
                # 3. æ™ºèƒ½é”™ä½ä¿®æ­£ (Data Shift Correction)
                is_shifted = False
                # å¦‚æœåºå·åˆ—å†…å®¹é•¿åº¦è¶…è¿‡5ä¸”ä¸æ˜¯çº¯æ•°å­—ï¼Œææœ‰å¯èƒ½æ˜¯é¡¹ç›®åç§°æŒ¤å äº†åºå·åˆ—
                if len(raw_idx_val) > 5 and not raw_idx_val.isdigit():
                    is_shifted = True
                
                item = {}
                if is_shifted:
                    # é”™ä½å¤„ç†ï¼šç‰©ç†åˆ—é‡æ˜ å°„
                    # å‡å®šç‰©ç†é¡ºåºåˆ—ï¼š[Name, Desc, Amount, SME, Time, Remark] (Indexä¸¢å¤±)
                    # å¼ºåˆ¶æŒ‰ç‰©ç†é¡ºåºè¯»å–
                    phy_cols = [c.get_text(" ", strip=True).replace("\n","").replace("\r","").strip() for c in cols]
                    # æ¸…æ´—ç‰©ç†åˆ—ä¸­çš„å¤šä½™ç©ºæ ¼
                    phy_cols = [" ".join(p.split()) for p in phy_cols]
                    while len(phy_cols) < 7: phy_cols.append("")
                    
                    item = {
                        "å­åºå·": "", 
                        "é‡‡è´­é¡¹ç›®åç§°": phy_cols[0],
                        "é‡‡è´­éœ€æ±‚æ¦‚å†µ": phy_cols[1],
                        "é¢„ç®—é‡‘é¢(ä¸‡å…ƒ)": phy_cols[2],
                        "æ‹Ÿé¢å‘ä¸­å°ä¼ä¸šé¢„ç•™": phy_cols[3],
                        "é¢„è®¡é‡‡è´­æ—¶é—´": phy_cols[4],
                        "å¤‡æ³¨": phy_cols[5] if len(phy_cols)>5 else ""
                    }
                else:
                    # æ­£å¸¸æ˜ å°„
                    item = {
                        "å­åºå·": raw_idx_val,
                        "é‡‡è´­é¡¹ç›®åç§°": raw_name_val,
                        "é‡‡è´­éœ€æ±‚æ¦‚å†µ": get_clean_text(col_map["desc"]),
                        "é¢„ç®—é‡‘é¢(ä¸‡å…ƒ)": get_clean_text(col_map["amount"]),
                        "æ‹Ÿé¢å‘ä¸­å°ä¼ä¸šé¢„ç•™": get_clean_text(col_map["sme_reserve"]),
                        "é¢„è®¡é‡‡è´­æ—¶é—´": get_clean_text(col_map["est_time"]),
                        "å¤‡æ³¨": get_clean_text(col_map["remark"])
                    }

                # 4. æœ‰æ•ˆæ€§æ ¡éªŒ
                if not item["é‡‡è´­é¡¹ç›®åç§°"] or item["é‡‡è´­é¡¹ç›®åç§°"] in ["é‡‡è´­é¡¹ç›®åç§°", "é¡¹ç›®åç§°", "åç§°"]:
                    continue
                
                # 5. å…¨å±€å»é‡ (ä½¿ç”¨ é¡¹ç›®åç§°+é‡‘é¢ ä½œä¸ºæŒ‡çº¹)
                unique_key = item["é‡‡è´­é¡¹ç›®åç§°"] + item["é¢„ç®—é‡‘é¢(ä¸‡å…ƒ)"]
                if unique_key in seen_titles:
                    continue
                seen_titles.add(unique_key)

                results.append(item)
                    
        return results

    def process_item(self, record):
        # record åŒ…å«åˆ—è¡¨é¡µå­—æ®µ: id, title, userName, areaName, date, buyKindCode...
        full_link = f"http://www.ccgp-shandong.gov.cn/detail?id={record['id']}&colCode={record['colCode']}&oldData={record['oldData']}"
        self._log(f"[{record.get('areaName', 'æœªçŸ¥')}] è§£æä¸­: {record.get('title', 'æ— æ ‡é¢˜')}")
        
        html = self.get_detail_html(record['id'], record['colCode'])
        child_rows = self.parse_html_table(html)
        
        final_rows = []
        
        # åŸºç¡€çˆ¶çº§å­—æ®µ (Parent Fields)
        parent_info = {
            "åœ°åŒº": record.get("areaName", ""),
            "æ ‡é¢˜": record.get("title", ""),
            "å‘å¸ƒäºº": record.get("publisher", ""),  # ä»è¯¦æƒ…é¡µæå–çš„å‘å¸ƒäºº
            # "é‡‡è´­æ–¹å¼": record.get("buyKindCode", ""),  # å®˜æ–¹æ•°æ®ä¸ºç©ºï¼Œå·²æ³¨é‡Š
            # "é¡¹ç›®ç±»å‹": record.get("projectType", ""),  # å®˜æ–¹æ•°æ®ä¸ºç©ºï¼Œå·²æ³¨é‡Š
            "å‘å¸ƒæ—¶é—´": record.get("date", ""),
            "Link": full_link
        }

        if child_rows:
            # æœ‰è¯¦æƒ…é¡µè¡¨æ ¼æ•°æ®ï¼šOne Parent -> Many Children
            for child in child_rows:
                row = parent_info.copy()
                row.update(child) # åˆå¹¶å­å­—æ®µ
                final_rows.append(row)
        else:
            # æ— è¯¦æƒ…é¡µè¡¨æ ¼æ•°æ®ï¼šOne Parent -> Empty Child (ä¿ç•™ä¸€è¡Œ)
            row = parent_info.copy()
            # å¡«å……ç©ºçš„å­å­—æ®µ
            row.update({
                "å­åºå·": "1",
                "é‡‡è´­é¡¹ç›®åç§°": record.get("title", ""), # å…œåº•ï¼šç”¨å¤§æ ‡é¢˜
                "é‡‡è´­éœ€æ±‚æ¦‚å†µ": "è¯¦æƒ…é¡µæœªè§£æåˆ°è¡¨æ ¼",
                "é¢„ç®—é‡‘é¢(ä¸‡å…ƒ)": "",
                "æ‹Ÿé¢å‘ä¸­å°ä¼ä¸šé¢„ç•™": "",
                "é¢„è®¡é‡‡è´­æ—¶é—´": "",
                "å¤‡æ³¨": ""
            })
            final_rows.append(row)
            
        return final_rows

    def run(self, max_pages=1, start_page=1, title="", start_time="", end_time="", area="370000"):
        from spider.browser_engine import BrowserEngine
        
        all_data = []
        self.browser = BrowserEngine(headless=False) # GUI æ¨¡å¼ä»¥ä¾¿é€šè¿‡éªŒè¯ç 
        self.browser.logger = self.log_func # ä¼ é€’æ—¥å¿—å‡½æ•°
        
        try:
            self.browser.init_driver()
            
            # 1. å¯¼èˆªå¹¶æœç´¢
            self.browser.goto_search_page()
            self.browser.perform_search(title, start_time, end_time, area)
            
            # 2. å¦‚æœèµ·å§‹é¡µä¸æ˜¯1ï¼Œè·³è½¬
            if start_page > 1:
                success = self.browser.jump_to_page(start_page)
                if not success:
                    self._log(f"è·³è½¬åˆ°ç¬¬ {start_page} é¡µå¤±è´¥ï¼Œå°†ä»å½“å‰é¡µå¼€å§‹")
            
            # 3. å¾ªç¯çˆ¬å–
            pages_crawled = 0
            current_page_idx = start_page
            
            while pages_crawled < max_pages:
                self._log(f"--- æ­£åœ¨å¤„ç†ç¬¬ {current_page_idx} é¡µ ---")
                
                # æå–åˆ—è¡¨ (æ— é™é‡è¯•æœºåˆ¶ï¼šç©ºç™½æ•°æ®ä¸€å®šæ˜¯éªŒè¯ç é—®é¢˜)
                records = self.browser.extract_records()
                
                rescue_attempts = 0
                max_rescue_attempts = 5  # âœ… æœ€å¤šé‡è¯•5æ¬¡éªŒè¯ç ,é¿å…æ— é™å¾ªç¯
                
                while not records and rescue_attempts < max_rescue_attempts:
                    rescue_attempts += 1
                    self._log(f"ç¬¬ {current_page_idx} é¡µæœªæ£€æµ‹åˆ°æ•°æ®ï¼Œæ‰§è¡ŒéªŒè¯ç é‡è¯• (ç¬¬ {rescue_attempts} æ¬¡)...")
                    
                    # é‡æ–°æ‰§è¡Œå…¨é‡æœç´¢é€»è¾‘ (Tab -> å‚æ•° -> åˆ·æ–°éªŒè¯ç  -> è¯†åˆ« -> æŸ¥è¯¢)
                    self.browser.perform_search(title, start_time, end_time, area)
                    
                    # æ£€æŸ¥å½“å‰é¡µç ï¼Œåªæœ‰ä¸åœ¨ç›®æ ‡é¡µæ—¶æ‰è·³è½¬
                    current_page_in_browser = self.browser.get_current_page()
                    if current_page_in_browser != current_page_idx:
                        self._log(f"å½“å‰é¡µç  {current_page_in_browser}ï¼Œéœ€è¦è·³è½¬åˆ°ç¬¬ {current_page_idx} é¡µ...")
                        self.browser.jump_to_page(current_page_idx)
                    else:
                        self._log(f"å½“å‰å·²åœ¨ç¬¬ {current_page_idx} é¡µï¼Œæ— éœ€è·³è½¬")
                    
                    # å†æ¬¡å°è¯•æå–
                    records = self.browser.extract_records()
                
                if not records:
                    self._log(f"âš ï¸ å·²é‡è¯• {max_rescue_attempts} æ¬¡éªŒè¯ç ä»æ— æ•°æ®")
                    
                    # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šç¬¬ä¸€é¡µæ— æ•°æ®ç›´æ¥é€€å‡º,è®¤ä¸ºä»Šæ—¥æ— æ•°æ®
                    if current_page_idx == start_page:
                        self._log(f"âœ… ç¬¬ä¸€é¡µåœ¨ {max_rescue_attempts} æ¬¡é‡è¯•åä»æ— æ•°æ®ï¼Œåˆ¤å®šä¸ºä»Šæ—¥æ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    # éç¬¬ä¸€é¡µåˆ™è·³è¿‡ç»§ç»­
                    self._log(f"è·³è¿‡ç¬¬ {current_page_idx} é¡µï¼Œç»§ç»­ä¸‹ä¸€é¡µ")
                    pages_crawled += 1
                    current_page_idx += 1
                    if not self.browser.next_page():
                        self._log("æ— æ³•ç‚¹å‡»ä¸‹ä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                        break
                    continue
                
                # è¯¦æƒ…é¡µå¤„ç† (ä¿æŒå¹¶å‘)
                # æ³¨æ„ï¼šBrowserEngine å·²ç»æå–äº† IDï¼Œæˆ‘ä»¬ç»§ç»­ç”¨ requests å¹¶å‘è·å–è¯¦æƒ…
                # ä¸ºäº†ä¿æŒ session çŠ¶æ€ (Cookies)ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®© requests ä½¿ç”¨ browser çš„ cookies
                # ä½†ç›®å‰è¯¦æƒ…é¡µ API ä¼¼ä¹ä¸éœ€è¦ cookie æˆ–è€…ä¸æ•æ„Ÿï¼Ÿ
                # å¦‚æœéœ€è¦ï¼Œå¯ä»¥: s = requests.Session(); s.cookies.update(...)
                
                if records:
                    with ThreadPoolExecutor(max_workers=2) as executor:
                        futures = [executor.submit(self.process_item, rec) for rec in records]
                        for f in futures:
                            res = f.result()
                            if res: all_data.extend(res)
                
                pages_crawled += 1
                if pages_crawled >= max_pages:
                    break
                
                # ç¿»é¡µ
                if not self.browser.next_page():
                    self._log("æ— æ³•ç‚¹å‡»ä¸‹ä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                    break
                    
                current_page_idx += 1
                
        except Exception as e:
            self._log(f"çˆ¬è™«è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            if self.browser:
                self._log("ä»»åŠ¡ç»“æŸï¼Œ5ç§’åè‡ªåŠ¨å…³é—­æµè§ˆå™¨...")
                time.sleep(5)
                self.browser.close()
                self.browser = None
                self._log("âœ… æµè§ˆå™¨å·²å…³é—­")
            
        return all_data

if __name__ == "__main__":
    s = Shandong()
    data = s.run(max_pages=2) # Test run
    df = pd.DataFrame(data)
    # Reorder columns
    cols = ["åºå·", "åˆ†ç±»1", "åˆ†ç±»2", "åœ°å¸‚", "å®¢æˆ·åç§°", "é¡¹ç›®åç§°", "é‡‘é¢", "é¢„è®¡æ—¶é—´", "link"]
    # Adjust åºå· to be global
    df['åºå·'] = range(1, len(df) + 1)
    df = df[cols]
    print(df.head())
    df.to_excel("shandong_bid.xlsx", index=False)
    print("Saved to shandong_bid.xlsx")
