import requests
import json
import base64
import time
from bs4 import BeautifulSoup
import pandas as pd
import threading
from concurrent.futures import ThreadPoolExecutor

import random

class Shandong(object):
    def __init__(self, use_proxy=True):
        self.list_url = "http://www.ccgp-shandong.gov.cn:8087/api/website/site/getListByCode"
        self.detail_url = "http://www.ccgp-shandong.gov.cn:8087/api/website/site/getDetail"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        ]
        self.colCode = "2500" # æ”¿é‡‡æ„å‘
        
        self.use_proxy = use_proxy
        self.proxies = None
        if self.use_proxy:
            self.proxies = {
                "http": "http://127.0.0.1:7897",
                "https": "http://127.0.0.1:7897",
            }
        
        self.log_func = None
        
        # ä»…åœ¨å¯ç”¨ä»£ç†æ—¶æ£€æŸ¥çŠ¶æ€
        if self.use_proxy:
            self.check_proxy()
        else:
            self._log("="*50)
            self._log("âš ï¸ ä»£ç†å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°ç›´æ¥è¿æ¥ã€‚")
            self._log("="*50)

    def check_proxy(self):
        """æ£€æŸ¥ä»£ç†æ˜¯å¦ç”Ÿæ•ˆå¹¶è·å–å‡ºå£IPä½ç½®"""
        self._log("="*50)
        self._log("æ­£åœ¨æ£€æŸ¥ç½‘ç»œå‡ºå£ç¯å¢ƒ...")
        test_url = "http://ip-api.com/json?lang=zh-CN"
        proxies = self.proxies
        
        try:
            # 1. è·å–ä»£ç†å‡ºå£ä¿¡æ¯
            resp = requests.get(test_url, proxies=proxies, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                ip = data.get("query")
                country = data.get("country", "")
                region = data.get("regionName", "")
                city = data.get("city", "")
                isp = data.get("isp", "")
                
                self._log(f"âœ… ä»£ç†å·²ç”Ÿæ•ˆï¼")
                self._log(f"   å½“å‰æ¢æµ‹å‡ºå£ IP: {ip}")
                self._log(f"   ç‰©ç†åœ°ç†ä½ç½®: {country} - {region} - {city}")
                self._log(f"   è¿è¥å•†ä¿¡æ¯: {isp}")
            else:
                self._log(f"âš ï¸ ä»£ç†è¿æ¥æµ‹è¯•è¿”å›çŠ¶æ€ç : {resp.status_code}")
        except Exception as e:
            self._log(f"âŒ ä»£ç†è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ Clash (127.0.0.1:7897) æ˜¯å¦å¼€å¯ã€‚")
            self._log(f"   é”™è¯¯è¯¦æƒ…: {e}")
        
        self._log("="*50)

    def _log(self, msg):
        if self.log_func:
            self.log_func(msg)
        else:
            print(msg)

    def get_headers(self):
        return {
            "accept": "application/json, text/plain, */*",
            # "accept-encoding": "gzip, deflate", # requests usually handles this
            "accept-language": "zh-CN,zh;q=0.9",
            "connection": "keep-alive",
            "content-type": "application/json;charset=UTF-8",
            "host": "www.ccgp-shandong.gov.cn:8087",
            "origin": "http://www.ccgp-shandong.gov.cn",
            "referer": "http://www.ccgp-shandong.gov.cn/",
            "user-agent": random.choice(self.user_agents)
        }
    def get_list(self, page, title="", start_time="", end_time="", area="370000"):
        # Date format must be YYYY-MM-DD HH:mm:ss
        if start_time and len(start_time) == 10:
            start_time += " 00:00:00"
        if end_time and len(end_time) == 10:
            end_time += " 23:59:59"
            
        data = {
            "colCode": self.colCode,
            "area": area,
            "currentPage": page,
            "pageSize": 10,
            "title": title,
            "projectCode": "",
            "buyKind": "",
            "buyType": "",
            "startTime": start_time if start_time else "",
            "oldData": 0,
            "endTime": end_time if end_time else "",
            "homePage": 0,
            "mergeType": 0
        }
        try:
            self._log(f"æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ (åœ°åŒº: {area}, æœç´¢è¯: {title})")
            # å¼ºåŒ–åçˆ¬ï¼šåˆ—è¡¨é¡µè¯·æ±‚é—´éš” 3.0 - 6.0 ç§’
            time.sleep(random.uniform(3.0, 6.0))
            
            resp = requests.post(self.list_url, json=data, headers=self.get_headers(), timeout=20, proxies=self.proxies)
            
            # çŠ¶æ€ç ç›‘æ§
            if resp.status_code in [403, 429]:
                self._log("ğŸ”¥ è­¦å‘Š: è§¦å‘æœåŠ¡å™¨æ‹¦æˆª (403/429)ï¼Œç«‹å³åœæ­¢çˆ¬å–ä»¥ä¿æŠ¤ IPï¼")
                return [], -1
            elif resp.status_code >= 500:
                self._log(f"ğŸ”¥ è­¦å‘Š: ç›®æ ‡æœåŠ¡å™¨è¿‡è½½æˆ–å‡ºé”™ (é”™è¯¯ç : {resp.status_code})ï¼Œåœæ­¢çˆ¬å–ï¼Œé¿å…åŠ é‡è´Ÿæ‹…ï¼")
                return [], -1
                
            if resp.status_code == 200:
                j = resp.json()
                # Assuming structure: j['data']['data']['records'] based on investigation
                # But test_api.py output showed j['data']['data'] has 'records'
                # Let's handle both just in case or stick to what we saw.
                if j.get("data") and j["data"].get("data") and j["data"]["data"].get("records"):
                    return j["data"]["data"]["records"], j["data"]["data"].get("pages", 0)
                else:
                    self._log("Debug - API JSON structure: " + json.dumps(j, indent=2, ensure_ascii=False))
            else:
                self._log(f"List error page {page}, status {resp.status_code}: {resp.text}")
        except Exception as e:
            self._log(f"List exception page {page}: {e}")
        return [], 0

    def get_detail_html(self, id_val, colCode):
        params = {
            "id": id_val,
            "colCode": colCode,
            "oldData": 0
        }
        try:
            # å¼ºåŒ–åçˆ¬ï¼šè¯¦æƒ…é¡µè¯·æ±‚é—´éš” 2.0 - 5.0 ç§’
            time.sleep(random.uniform(2.0, 5.0))
            resp = requests.get(self.detail_url, params=params, headers=self.get_headers(), timeout=20, proxies=self.proxies)
            
            if resp.status_code in [403, 429]:
                self._log(f"ğŸ”¥ è¯¦æƒ…é¡µ {id_val} è§¦å‘æ‹¦æˆªï¼Œè·³è¿‡...")
                return None
                
            if resp.status_code == 200:
                j = resp.json()
                if j.get("data") and j["data"].get("data") and j["data"]["data"].get("body"):
                    body = j["data"]["data"]["body"]
                    try:
                        return base64.b64decode(body).decode('utf-8')
                    except:
                        try:
                            return base64.b64decode(body).decode('gb18030')
                        except:
                            return None
        except Exception as e:
            self._log(f"Detail exception {id_val}: {e}")
        return None

    def parse_html_table(self, html):
        if not html:
            return []
        soup = BeautifulSoup(html, 'lxml')
        tables = soup.find_all('table')
        results = []
        for table in tables:
            # Check headers
            rows = table.find_all('tr')
            if not rows:
                continue
            
            headers = [th.get_text(strip=True) for th in rows[0].find_all(['td', 'th'])]
            # Looking for headers like "é¡¹ç›®åç§°", "é‡‘é¢", "æ—¶é—´"
            # Typical headers: åºå·, é¡¹ç›®åç§°, é‡‡è´­éœ€æ±‚æ¦‚å†µ, é¢„ç®—é‡‘é¢(ä¸‡å…ƒ), é¢„è®¡é‡‡è´­æ—¶é—´, å¤‡æ³¨
            name_idx = -1
            amt_idx = -1
            time_idx = -1
            
            for i, h in enumerate(headers):
                if "é¡¹ç›®åç§°" in h:
                    name_idx = i
                elif "é‡‘é¢" in h or "é¢„ç®—" in h:
                    amt_idx = i
                elif "æ—¶é—´" in h:
                    time_idx = i
            
            if name_idx != -1:
                # Valid table
                for row in rows[1:]:
                    cols = row.find_all('td')
                    if len(cols) > max(name_idx, amt_idx, time_idx):
                        item = {}
                        item['project_name'] = cols[name_idx].get_text(strip=True)
                        item['amount'] = cols[amt_idx].get_text(strip=True) if amt_idx != -1 else ""
                        item['est_time'] = cols[time_idx].get_text(strip=True) if time_idx != -1 else ""
                        results.append(item)
        return results

    def process_item(self, record):
        # record has id, title, userName (client name), areaName (city), date, colCode
        full_link = f"http://www.ccgp-shandong.gov.cn/detail?id={record['id']}&colCode={record['colCode']}&oldData={record['oldData']}"
        self._log(f"[{record.get('areaName', 'æœªçŸ¥')}] æ­£åœ¨çˆ¬å–è¯¦æƒ…: {record.get('title', 'æ— æ ‡é¢˜')}")
        self._log(f"   URL: {full_link}")
        
        html = self.get_detail_html(record['id'], record['colCode'])
        projects = self.parse_html_table(html)
        
        final_rows = []
        if projects:
            for i, p in enumerate(projects):
                row = {
                    "åºå·": i + 1, # relative to list, or just global? User example "1"
                    "åˆ†ç±»1": "",
                    "åˆ†ç±»2": "æ”¿é‡‡æ„å‘",
                    "åœ°å¸‚": record.get("areaName", ""),
                    "å®¢æˆ·åç§°": record.get("userName", ""),
                    "é¡¹ç›®åç§°": p['project_name'],
                    "é‡‘é¢": p['amount'],
                    "é¢„è®¡æ—¶é—´": p['est_time'],
                    "link": full_link
                }
                final_rows.append(row)
        else:
            # If no table found, maybe correct logic is different or it's a unstructured text.
            # Fallback: use title as project name?
            row = {
                "åºå·": 1,
                "åˆ†ç±»1": "",
                "åˆ†ç±»2": "æ”¿é‡‡æ„å‘",
                "åœ°å¸‚": record.get("areaName", ""),
                "å®¢æˆ·åç§°": record.get("userName", ""),
                "é¡¹ç›®åç§°": record.get("title", ""),
                "é‡‘é¢": "",
                "é¢„è®¡æ—¶é—´": "",
                "link": full_link
            }
            final_rows.append(row)
        return final_rows

    def run(self, max_pages=5, title="", start_time="", end_time="", area="370000"):
        all_data = []
        # First get total pages
        records, total_pages = self.get_list(1, title, start_time, end_time, area)
        self._log(f"Total pages available: {total_pages}, Processing max: {max_pages}")
        
        pages_to_crawl = min(total_pages, max_pages)
        if pages_to_crawl == 0 and len(records) > 0:
             # Case where total_pages might be 0 but data exists? 
             # Usually pages >= 1 if records > 0
             pages_to_crawl = 1
        
        # é™ä½å¹¶å‘ï¼šmax_workers ä» 5 é™è‡³ 2
        if records:
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = [executor.submit(self.process_item, rec) for rec in records]
                for f in futures:
                    res = f.result()
                    if res: all_data.extend(res)
        
        # å¤„ç†åç»­é¡µé¢
        for p in range(2, pages_to_crawl + 1):
            self._log(f"--- å‡†å¤‡ç¿»é˜…ç¬¬ {p} é¡µ ---")
            # é¡µé™…é•¿ä¼‘çœ ï¼š3.0 - 8.0 ç§’
            time.sleep(random.uniform(3.0, 8.0))
            
            records, total = self.get_list(p, title, start_time, end_time, area)
            if total == -1: break # è§¦å‘ç†”æ–­
            
            if records:
                with ThreadPoolExecutor(max_workers=2) as executor:
                    futures = [executor.submit(self.process_item, rec) for rec in records]
                    for f in futures:
                        res = f.result()
                        if res: all_data.extend(res)
            
        return all_data

if __name__ == "__main__":
    s = Shandong()
    data = s.run(max_pages=2) # Test run
    df = pd.DataFrame(data)
    # Reorder columns
    cols = ["åºå·", "åˆ†ç±»1", "åˆ†ç±»2", "åœ°å¸‚", "å®¢æˆ·åç§°", "é¡¹ç›®åç§°", "é‡‘é¢", "é¢„è®¡æ—¶é—´", "link"]
    # Adjust åºå· to be global
    df['åºå·'] = range(1, len(df) + 1)
    df = df[cols]
    print(df.head())
    df.to_excel("shandong_bid.xlsx", index=False)
    print("Saved to shandong_bid.xlsx")
